
@book{Chomsky1995,
  title={The Minimalist Program},
  author={Chomsky, Noam},
  isbn={9780262531283},
  series={Current studies in linguistics series},
  year={1995},
  publisher={MIT Press}
}

@book{Chomsky2014minimalist,
  title={The minimalist program},
  author={Chomsky, Noam},
  year={2014}
}


% pulvermuller 1999a
@article{Pulvermuller1999,
title = "Words in the brain's language",
journal = "Behavioural Brain Science",
volume = "22",
number = "2",
pages = "253--79",
year = "1999",
author = "Friedemann Pulverm{\"u}ller",
}


% pulvermuller 1999a
@article{PulverMullerEtAl1999,
    author = {Pulvermüller, Friedemann and Lutzenberger, Werner and Preissl, Hubert},
    title = "{Nouns and verbs in the intact brain: evidence from event-related potentials and high-frequency cortical responses}",
    journal = {Cerebral Cortex},
    volume = {9},
    number = {5},
    pages = {497--506},
    year = {1999},
    month = {07},
    abstract = "{Lesion evidence indicates that words from different lexical categories, such as nouns and verbs, may have different cortical counterparts. In this study, processing of nouns and verbs was investigated in the intact brain using (i) behavioral measures, (ii) stimulus-triggered event-related potentials and (iii) high-frequency electrocortical responses in the gamma band. Nouns and verbs carefully matched for various variables, including word frequency, length, arousal and valence, were presented in a lexical decision task while electrocortical responses were recorded. In addition, information about cognitive processing of these stimuli was obtained using questionnaires and reaction times. As soon as ~200 ms after stimulus onset, event-related potentials disclosed electrocortical differences between nouns and verbs over widespread cortical areas. In a later time window, 500–800 ms after stimulus onset, there was a significant difference between high-frequency responses in the 30 Hz range. Difference maps obtained from both event-related potentials and high-frequency responses revealed strong betweencategory differences of signals recorded above motor and visual cortices. Behavioral data suggest that these different physiological responses are related to semantic associations (motor or visual) elicited by these word groups. Our results are consistent with a neurobiological model of language representation postulating cell assemblies with distinct cortical topographies as biological counterparts of words. Assemblies representing nouns referring to visually perceived objects may include neurons in visual cortices, and assemblies representing action verbs may include additional neurons in motor, premotor and prefrontal cortices. Event-related potentials and high-frequency responses are proposed to indicate two different functional states of cell assemblies: initial full activation (‘ignition’) and continuous reverberatory activity.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/9.5.497},
    url = {https://doi.org/10.1093/cercor/9.5.497},
    eprint = {http://oup.prod.sis.lan/cercor/article-pdf/9/5/497/9752730/090497.pdf},
}




% Pulvermuller, 2002
@article{PulvermullerEtAl2002,
title = "A brain perspective on language mechanisms: from discrete neuronal ensembles to serial order",
journal = "Progress in Neurobiology",
volume = "67",
number = "2",
pages = "85--111",
year = "2002",
issn = "0301-0082",
doi = "10.1016/S0301-0082(02)00014-X",
author = "Friedemann Pulverm{\"u}ller",
abstract = "Language is constituted by discrete building blocks, sounds and words, which can be concatenated according to serial order principles. The neurobiological organization of these building blocks, in particular words, has been illuminated by recent metabolic and neurophysiological imaging studies. When humans process words of different kinds, various sets of cortical areas have been found to become active differentially. The old concept of two language centers processing all words alike must therefore be replaced by a model according to which words are organized as discrete distributed neuron ensembles that differ in their cortical topographies. The meaning of a word, more precisely, aspects of its reference, may be crucial for determining which set of cortical areas becomes involved in its processing. Whereas the serial order of sounds constituting a word may be established by serially aligned sets of neurons called synfire chains, different mechanisms are necessary for establishing word order in sentences. The serial order of words may be organized by higher-order neuronal sets, called sequence detectors here, which are being activated by sequential excitation of neuronal sets representing words. Sets of sequence detectors are proposed to process aspects of the syntactic information contained in a sentence. Other syntactic rules can be related to general features of the dynamics of cortical activation and deactivation. These postulates about the brain mechanisms of language, which are rooted in principles known from neuroanatomy and neurophysiology, may provide a framework for theory-driven neuroscientific research on language."
}


@article{SanfordSturt2002,
title = "Depth of processing in language comprehension: not noticing the evidence",
journal = "Trends in Cognitive Sciences",
volume = "6",
number = "9",
pages = "382--386",
year = "2002",
issn = "1364-6613",
doi = "10.1016/S1364-6613(02)01958-7",
url = "www.sciencedirect.com/science/article/pii/S1364661302019587",
author = "Anthony J. Sanford and Patrick Sturt",
keywords = "Language interpretation, Underspecification, Text-change-blindness, Semantic anomalies, Representation",
abstract = "The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding."
}

@article{FrankBod2011,
title ={Insensitivity of the human sentence-processing system to hierarchical structure},
journal = {Psychological Science},
volume = {22},
number = {6},
pages = {829--834},
year = {2011},
doi = {10.1177/0956797611409589},
note ={PMID: 21586764},
author = {Frank, Stefan L. and Bod, Rens},
abstract = { Although it is generally accepted that hierarchical phrase structures are instrumental in describing human language, their role in cognitive processing is still debated. We investigated the role of hierarchical structure in sentence processing by implementing a range of probabilistic language models, some of which depended on hierarchical structure, and others of which relied on sequential structure only. All models estimated the occurrence probabilities of syntactic categories in sentences for which reading-time data were available. Relating the models’ probability estimates to the data showed that the hierarchical-structure models did not account for variance in reading times over and above the amount of variance accounted for by all of the sequential-structure models. This suggests that a sentence’s hierarchical structure, unlike many other sources of information, does not noticeably affect the generation of expectations about upcoming words. }
}


% Fieldtrip
@article{Fieldtrip,
title = "Open Source Software for Advanced Analysis of {MEG}, {EEG}, and Invasive Electrophysiological Data",
journal = "Computational Intelligence and Neuroscience",
year = "2011",
author = "Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs",
doi={10.1155/2011/156869}
}

% Frank, 2012
@article{FrankEtAl2012,
title = "How hierarchical is language use?",
journal = "Proceedings in Biological Sciences",
volume = "272",
number = "1747",
pages = "4522--4531",
year = "2012",
author = "Frank, S. L. and Bod, R and Christiansen, M. H.",
doi={10.1098/rspb.2012.1741}
}


% Mikolov2013
@incollection{MikolovEtAl2013,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.}
}


@article{DBLP:journals/corr/MikolovSCCD13,
  author    = {Tomas Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  journal   = {CoRR},
  volume    = {abs/1310.4546},
  year      = {2013},
  url       = {http://arxiv.org/abs/1310.4546},
  archivePrefix = {arXiv},
  eprint    = {1310.4546},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MikolovSCCD13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{MikolovEtAl2013b,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  volumn={1301.3781},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013},
  URL	= {http://arxiv.org/abs/1301.3781}
}



@inproceedings{MikolovEtAl2013c,
  title={Linguistic regularities in continuous space word representations},
  author={Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={746--751},
  year={2013}
}

@article{BerwickEtAl2013,
  title={Evolution, brain, and the nature of language},
  author={Berwick, Robert C and Friederici, Angela D and Chomsky, Noam and Bolhuis, Johan J},
  journal={Trends in Cognitive Sciences},
  volume={17},
  number={2},
  pages={89--98},
  year={2013},
  publisher={Elsevier},
  doi= "10.1016/j.tics.2012.12.002"
}



% Everaert 2015
@article{EveraertEtAl2015,
title = "Structures, not strings: linguistics as part of the cognitive sciences",
journal = "Trends in Cognitive Sciences",
volume = "19",
number = "12",
pages = "729--743",
year = "2015",
issn = "1364-6613",
doi = "10.1016/j.tics.2015.09.008",
author = "Martin B.H. Everaert and Marinus A.C. Huybregts and Noam Chomsky and Robert C. Berwick and Johan J. Bolhuis",
abstract = "There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what ‘language’ is. Sharpening the core question – what is language? – and paying close attention to the basic property of the language faculty and its biological foundations makes it clear how linguistics is firmly positioned within the cognitive sciences. Here we will show how recent developments in generative grammar, taking language as a computational cognitive mechanism seriously, allow us to address issues left unexplained in the increasingly popular surface-oriented approaches to language."
}





% Christiansen and Chater, 2016
@article{ChristiansenChater2016, title={The Now-or-Never bottleneck: A fundamental constraint on language}, volume={39}, DOI={10.1017/S0140525X1500031X}, journal={Behavioral and Brain Sciences}, publisher={Cambridge University Press}, author={Christiansen, Morten H. and Chater, Nick}, year={2016}, pages={e62}}

% Ding 2016
@article{DingEtAl2016,
author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
year = {2016},
title = {Cortical tracking of hierarchical linguistic structures in connected speech},
volume = {19},
number=1,
pages={158--164}, 
journal = {Nature Neuroscience},
doi = {10.1038/nn.4186}
}


% Federenko 2016
@article {FedorenkoE6256,
	author = {Fedorenko, Evelina and Scott, Terri L. and Brunner, Peter and Coon, William G. and Pritchett, Brianna and Schalk, Gerwin and Kanwisher, Nancy},
	title = {Neural correlate of the construction of sentence meaning},
	volume = {113},
	number = {41},
	pages = {E6256--E6262},
	year = {2016},
	doi = {10.1073/pnas.1612132113},
	publisher = {National Academy of Sciences},
	abstract = {How do circuits of neurons in your brain extract and hold the meaning of a sentence? To start to address this unanswered question, we measured neural activity from the surface of the human brain in patients being mapped out before neurosurgery, as they read sentences. In many electrodes, neural activity increased steadily over the course of the sentence, but the same was not found when participants read lists of words or pronounceable nonwords, or grammatical nonword strings ({\textquotedblleft}Jabberwocky{\textquotedblright}). This build-up of neural activity appears to reflect neither word meaning nor syntax alone, but the representation of complex meanings.The neural processes that underlie your ability to read and understand this sentence are unknown. Sentence comprehension occurs very rapidly, and can only be understood at a mechanistic level by discovering the precise sequence of underlying computational and neural events. However, we have no continuous and online neural measure of sentence processing with high spatial and temporal resolution. Here we report just such a measure: intracranial recordings from the surface of the human brain show that neural activity, indexed by γ-power, increases monotonically over the course of a sentence as people read it. This steady increase in activity is absent when people read and remember nonword-lists, despite the higher cognitive demand entailed, ruling out accounts in terms of generic attention, working memory, and cognitive load. Response increases are lower for sentence structure without meaning ({\textquotedblleft}Jabberwocky{\textquotedblright} sentences) and word meaning without sentence structure (word-lists), showing that this effect is not explained by responses to syntax or word meaning alone. Instead, the full effect is found only for sentences, implicating compositional processes of sentence understanding, a striking and unique feature of human language not shared with animal communication systems. This work opens up new avenues for investigating the sequence of neural events that underlie the construction of linguistic meaning.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/113/41/E6256},
	eprint = {https://www.pnas.org/content/113/41/E6256.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


%Bojanowski 2017
@article{Bojanowski2017,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  year={2017},
  issn={2307-387X},
  pages={135--146},
  doi = {10.1162/tacl\_a\_00051},
}


% Ding 2017
@article{DingEtAl2017,
author={Ding, Nai and Melloni, Lucia and Yang, Aotian and Wang, Yu and Zhang, Wen and Poeppel, David},   
title={Characterizing neural entrainment to hierarchical linguistic units using electroencephalography ({EEG})},      	
journal={Frontiers in Human Neuroscience},      	
VOLUME={11},      
pages={481},    
year={2017},      	  
doi={10.3389/fnhum.2017.00481},      
issn={1662-5161},    
}


@Article{Jackendoff2017,
author="Jackendoff, Ray
and Wittenberg, Eva",
title="Linear grammar as a possible stepping-stone in the evolution of language",
journal="Psychonomic Bulletin {\&} Review",
year="2017",
month="Feb",
day="01",
volume="24",
number="1",
pages="219--224",
abstract="We suggest that one way to approach the evolution of language is through reverse engineering: asking what components of the language faculty could have been useful in the absence of the full complement of components. We explore the possibilities offered by linear grammar, a form of language that lacks syntax and morphology altogether, and that structures its utterances through a direct mapping between semantics and phonology. A language with a linear grammar would have no syntactic categories or syntactic phrases, and therefore no syntactic recursion. It would also have no functional categories such as tense, agreement, and case inflection, and no derivational morphology. Such a language would still be capable of conveying certain semantic relations through word order---for instance by stipulating that agents should precede patients. However, many other semantic relations would have to be based on pragmatics and discourse context. We find evidence of linear grammar in a wide range of linguistic phenomena: pidgins, stages of late second language acquisition, home signs, village sign languages, language comprehension (even in fully syntactic languages), aphasia, and specific language impairment. We also find a full-blown language, Riau Indonesian, whose grammar is arguably close to a pure linear grammar. In addition, when subjects are asked to convey information through nonlinguistic gesture, their gestures make use of semantically based principles of linear ordering. Finally, some pockets of English grammar, notably compounds, can be characterized in terms of linear grammar. We conclude that linear grammar is a plausible evolutionary precursor of modern fully syntactic grammar, one that is still active in the human mind.",
issn="1531-5320",
doi="10.3758/s13423-016-1073-y",
url="https://doi.org/10.3758/s13423-016-1073-y"
}


% Nelson 2017
@article {NelsonEtAl2017,
	author = {Nelson, Matthew J. and El Karoui, Imen and Giber, Kristof and Yang, Xiaofang and Cohen, Laurent and Koopman, Hilda and Cash, Sydney S. and Naccache, Lionel and Hale, John T. and Pallier, Christophe and Dehaene, Stanislas},
	title = {Neurophysiological dynamics of phrase-structure building during sentence processing},
	volume = {114},
	number = {18},
	pages = {E3669--E3678},
	year = {2017},
	doi = {10.1073/pnas.1701590114},
	publisher = {National Academy of Sciences},
	abstract = {According to most linguists, the syntactic structure of sentences involves a tree-like hierarchy of nested phrases, as in the sentence [happy linguists] [draw [a diagram]]. Here, we searched for the neural implementation of this hypothetical construct. Epileptic patients volunteered to perform a language task while implanted with intracranial electrodes for clinical purposes. While patients read sentences one word at a time, neural activation in left-hemisphere language areas increased with each successive word but decreased suddenly whenever words could be merged into a phrase. This may be the neural footprint of merge},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/114/18/E3669},
	eprint = {https://www.pnas.org/content/114/18/E3669.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{MartinDoumas2017,
    author = {Martin, Andrea E. AND Doumas, Leonidas A. A.},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {A mechanism for the cortical computation of hierarchical linguistic structure},
    year = {2017},
    month = {03},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pbio.2000663},
    pages = {1--23},
    abstract = {Author summary Human language is a fundamental biological signal with computational properties that differ from other perception-action systems: hierarchical relationships between sounds, words, phrases, and sentences and the unbounded ability to combine smaller units into larger ones, resulting in a "discrete infinity" of expressions. These properties have long made language hard to account for from a biological systems perspective and within models of cognition. We argue that a single computational mechanism—using time to encode hierarchy—can satisfy the computational requirements of language, in addition to those of other cognitive functions. We show that a well-supported neural network model of analogy oscillates like the human brain while processing sentences. Despite being built for an entirely different purpose (learning relational concepts), the model processes hierarchical representations of sentences and exhibits oscillatory patterns of activation that closely resemble the human cortical response to the same stimuli. From the model, we derive an explicit computational mechanism for how the human brain could convert perceptual features into hierarchical representations across multiple timescales, providing a linking hypothesis between linguistic and cortical computation. Our results suggest a formal and mechanistic alignment between representational structure building and cortical oscillations that has broad implications for discovering the computational first principles of cognition in the human brain.},
    number = {3},
    doi = {10.1371/journal.pbio.2000663}
}

@article{FrankChristiansen2018,
author = {Stefan L. Frank and Morten H. Christiansen},
title = {Hierarchical and sequential processing of language},
journal = {Language, Cognition and Neuroscience},
volume = {33},
number = {9},
pages = {1213--1218},
year  = {2018},
publisher = {Routledge},
doi = {10.1080/23273798.2018.1424347}
}

% Frank 2018
@article{FrankYang2018,
    author = {Frank, Stefan L. AND Yang, Jinbiao},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Lexical representation explains cortical entrainment during speech comprehension},
    year = {2018},
    volume = {13},
    pages = {1--11},
    abstract = {Results from a recent neuroimaging study on spoken sentence comprehension have been interpreted as evidence for cortical entrainment to hierarchical syntactic structure. We present a simple computational model that predicts the power spectra from this study, even though the model’s linguistic knowledge is restricted to the lexical level, and word-level representations are not combined into higher-level units (phrases or sentences). Hence, the cortical entrainment results can also be explained from the lexical properties of the stimuli, without recourse to hierarchical syntax.},
    number = {5},
    doi = {10.1371/journal.pone.0197304}
}

@article{Meyer2018,
author = {Meyer, Lars},
title = {The neural oscillations of speech processing and language comprehension: state of the art and emerging mechanisms},
journal = {European Journal of Neuroscience},
volume = {48},
number = {7},
pages = {2609--2621},
keywords = {chunking, entrainment, memory, predictive coding},
doi = {10.1111/ejn.13748},
year = {2018}
}

@article{WarrenEtAl1969,
  title={Auditory sequence: Confusion of patterns other than speech or music},
  author={Warren, Richard M and Obusek, Charles J and Farmer, Richard M and Warren, Roslyn P},
  journal={Science},
  volume={164},
  number={3879},
  pages={586--587},
  year={1969},
  publisher={American Association for the Advancement of Science}
}

% Praat software
@misc{Praat,
  author = {Boersma, Paul and Weenink, David},
  title = {Praat: doing phonetics by computer [Computer program]},
  howpublished = {\texttt{www.praat.org}},
  note = {Version 6.0.56, retrieved 20 June 2019},
  year = {2019},
  volumn = "v6.0.56"
}


@article{Baroni2019,
  title={Linguistic generalization and compositionality in modern artificial neural networks},
  author={Baroni, Marco},
  journal={arXiv preprint arXiv:1904.00157 to appear in the Philosophical Transactions of the Royal Society B},
  year={2019}
}


@article{Baroni2019linguistic,
  author    = {Marco Baroni},
  title     = {Linguistic generalization and compositionality in modern artificial neural networks},
  journal   = {Computing Research Repository},
  volume    = {abs/1904.00157},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1904.00157},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-00157},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{LakretzEtAl2019,
  author    = {Yair Lakretz and
               Germ{\'{a}}n Kruszewski and
               Theo Desbordes and
               Dieuwke Hupkes and
               Stanislas Dehaene and
               Marco Baroni},
  title     = {The emergence of number and syntax units in {LSTM} language models},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,{NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
  pages     = {11--20},
  year      = {2019}
}

% BUT BARONI CITES THIS PAPER HIMSELF AS:
%Yair  Lakretz,  Germ ́an  Kruszewski,  Theo  Desbordes,  Dieuwke  Hupkes,Stanislas Dehaene, and Marco Baroni.  The emergence of number and syntax  units  in  LSTM  language  models.   In Proceedings  of  NAACL,  pages 11–20, Minneapolis, MN, 2019
